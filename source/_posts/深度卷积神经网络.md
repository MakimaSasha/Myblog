---
title: 深度卷积神经网络
date: 2023-03-06 0:34:68
tag: 机器学习
---

## 卷积层

### 功能

```python
	提取图像特征
```

### 引入原因

```python
	卷积层具有平移不变性即：特征与其所在的位置无关，不同位置上相同的特征具有相同响应，除此以外，卷积核的参数具有共享性，即卷积核的参数在整个特征图上共享（因为每一个卷积核都会遍历整个特征图），这样大大减少的参数的数量，提高模型的泛化能力（就是模型在训练集中学习到的知识能够在新的数据集或实际应用中取得好的表现）
```

### 原理

```python
卷积核是一个小的矩阵，通常由随机初始化（从一些分布中随机采样的值，比如从标准高斯分布中随机采样9个数，组成一个3x3的卷积核）的权重组成。卷积核的大小通常比输入小，例如3x3或5x5
	滑动窗口：将卷积核在输入上滑动，每次取一个小的局部区域，与卷积核进行点乘操作，得到一个标量值（卷积核上的一个元素和每次所取的小区域上的对应位置上元素相乘）
	卷积操作：将点乘得到的标量值累加起来，得到卷积操作的结果，作为输出的一个元素
	重复操作：重复上述过程，直到遍历完整个输入，得到输出的所有元素
需要注意的是，在卷积操作中，卷积核的权重是可训练的参数，通过反向传播算法来更新权重，以使得卷积层可以自适应地提取输入中的特征
```

### 填充操作

```python
为了保持特征图的大小不变，通常需要进行填充操作。填充是在输入特征图的周围添加一圈0值像素，以便在进行卷积操作时，卷积核能够覆盖到输入的边缘像素
填充的主要作用有以下几个方面：
	1.保持特征图大小不变：在卷积操作中，如果不进行填充操作，卷积核只能对输入特征图的中心像素进行卷积操作，导致输出特征图的大小比输入特征图小。通过进行填充操作，可以保持特征图的大小不变，使得特征图中的所有像素都能够被卷积核覆盖到
	2.提高特征图的稳定性：在进行卷积操作时，如果输入特征图的边缘像素没有被卷积核覆盖到，那么它们的输出值将会很小，对整个特征图的贡献也很小。通过进行填充操作，可以使得输入特征图的边缘像素也能够被卷积核覆盖到，提高特征图的稳定性
	3.提高特征提取能力：填充操作可以扩大输入特征图的有效区域，使得卷积核能够提取更多的局部特征，从而提高特征提取的能力
需要注意的是，填充操作会增加计算量和存储量，因此在实际应用中需要根据具体情况选择合适的填充方式和填充大小
```

### 卷积操作

```python
1.valid卷积：valid卷积是指在进行卷积操作时，不进行填充操作，只对输入和卷积核重叠部分进行卷积操作，输出特征图的大小比输入特征图小

2.same卷积：same卷积是指在进行卷积操作时，使用0值填充使得输出特征图的大小与输入特征图的大小相同。就是padding=1,stride=1,卷积核大小为3x3

3.膨胀卷积：膨胀卷积是一种卷积操作，它在卷积核内部添加了一个固定的膨胀因子，使得卷积核的有效区域扩大，从而能够提取更多的局部特征

4.反卷积：反卷积是一种卷积操作的逆运算，它可以将卷积操作得到的低维特征图还原为高维特征图。在反卷积操作中，通常需要使用插值和填充等技术来还原输入特征图的大小和形状

这些卷积操作在卷积神经网络中都有着广泛的应用。其中，valid卷积和same卷积可以控制输出特征图的大小和形状，膨胀卷积可以扩大卷积核的有效区域，提高特征提取的能力，反卷积可以还原低维特征图为高维特征图，从而实现特征的重建和还原
```

### 卷积层输出

```python
卷积后输出图的大小为W*H：
	W = (w - k + 2*p)/s + 1
	H = (h - k + 2*p)/s + 1
#说明：
    1.卷积层的输入图片大小为w*h，（一般输入图片都是正方形的）
    2.卷积核的大小为k*k
    3.填充大小为p
    4.步长为s
    5.除不尽时只保留整数部分
```

### 示例代码

```python
 network = input_data(shape=[None, width, height, 3], name='input')	#shape即输入的图片形状，None表示可以接受任意大小的输入数据，width数据的宽度，heigh数据的高度，3表示通道数，因为一般图片都是RGB格式，通道为3。第一输入数据且命名为input
    network = conv_2d(network, 96, 11, strides=4, activation='relu')	#命名的input的数据进行卷积，卷积核数量为96，大小为11x11，步幅为4，且卷积层的激活函数为ReLu

#补充：
	卷积层后一般会跟一个池化层，因为卷积层是用来提取数据特征，而池化层是用来进行下采样减小特征图的大小以降低模型的计算复杂度
```

### 补充·通道相关

```python
1.使用1x1的卷积核进行通道融合，因为1x1的卷积核不会提取信息，此时指定输出通道数为n则会使特征图的通道数变为n
#例子:
    输入特征图：c1 h w	   #c1为输入特征图的通道数，h为高度，w为宽度
    n*卷积核：c1 k1 k2		#n为输出通道数，每一个输出通道包含一个卷积核，c1为每一个卷积核的通道数，k1为卷积核的高度，k2为卷积核的宽度
    输出特征图：n Oh Ow	   #n为输出的特征图的通道数，Oh为输出特征图的高度，Ow为输出特征图的宽度
    
2.通常在卷积之后，如果特征图缩小了一倍，则一般将输出通道设置为输入的一倍

3.每个通道的卷积核是不一样的，通道的作用是识别一个特征，不同的通道识别不同的特征
```

## Relu函数层

### 功能

```python
# Relu层输出值 = Relu(卷积层输出值)
	1.引入非线性：激活函数可以引入非线性变换，从而使得神经网络可以处理非线性问题。如果神经网络中没有激活函数，那么多个线性变换的组合仍然是线性变换，无法处理复杂的非线性问题
	2.压缩输出范围：激活函数可以将神经元的输出范围压缩到一个有限的区间内，例如[0,1]或[-1,1]，从而避免神经元输出过大或过小的问题
	3.提高网络的表达能力：激活函数可以使得神经网络具有更强的表达能力，能够处理更加复杂的问题。例如，ReLU函数可以缓解神经元死亡问题，提高网络的表达能力
	4.提高模型的鲁棒性：激活函数可以增加模型的鲁棒性，使得模型对输入数据的微小变化不敏感。例如，Sigmoid函数的输出在0附近变化很快，因此能够对输入数据的微小变化产生较强的响应，提高模型的鲁棒性
    
##### 实质：如果不引入激活函数，那么神经元的输出就只会是线性结果，而大多数情况下需要的都是非线性的，因此引入激活函数将数据进行非线性化
```

### 优点

```python
	在深度神经网络中，ReLU函数是一种更加常用的激活函数，因为ReLU函数具有简单、易于计算、不会出现梯度消失等优点
```

### 其他

```python
	Sigmoid函数，是一种常用的非线性函数。由于Sigmoid函数的导数具有“梯度消失”现象，即在函数的两端，导数的值趋近于0，导致梯度无法传递，因此在深度神经网络中，Sigmoid函数不再是一种常用的激活函数
```

## 池化层

```python
#作用：
对输入特征图进行下采样，从而减少特征图的大小和参数量，提高模型的鲁棒性和泛化能力，减少计算量。类似缩略图，如：从左到右、从上到下从3x3的局部小块中将这9个元素中的最大值作为特征保留，直至处理完整个输入，输出的图就变小了很多

#池化操作
最大池化是一种常用的池化操作，它可以通过将输入数据分割成固定大小的矩形区域，并在每个区域内选择最大值来减少数据的维度。最大池化通常用于卷积神经网络中，以减小特征图的大小，并提取最显著的特征。最大池化的优点包括：
    1.减小数据维度，降低模型复杂度
    2.提取最显著的特征，增强模型的鲁棒性
    3.减少过拟合的风险，提高模型的泛化能力

最大池化的缺点包括：
可能会丢失一些信息，因为只选择每个区域内的最大值
可能会引入一些不必要的噪声，因为最大值可能受到局部噪声的影响。
总之，最大池化是一种非常有用的操作，可以帮助卷积神经网络提取最显著的特征，并降低数据维度，从而提高模型的效率和鲁棒性
    1.最大池化会选取每个池化区域中的最大值作为输出
    2.平均池化则是取池化区域内的平均值作为输出

#池化后输出图的大小为W*H：
	W = (w - F)/s + 1
	H = (h - F)/s + 1
    #说明：
        1.池化层的输入图片大小为w*h
        2.池化核的大小为F*F
        3.步长为s
        4.除不尽时只保留整数部分


#示例代码：
pool1_3_3 = max_pool_2d(conv1, 3, strides=2)	#将卷积后的特征数据conv1进行最大池化操作，池化窗口大小为3x3，即每次从9个数中选择最大的一个值作为输出特征图中的一个数据，池化窗口的步长为2，即每个池化窗口最左边相距的间隔为2
```

## 规范层

### 功能

```python
	规范层（Normalization Layer）是一种在深度学习中常用的正则化方法，它的作用是对网络中的数据进行归一化处理，以减少过拟合的风险和提高模型的鲁棒性。就是将数据的取值映射到一定的区间中。
    作用在于减少网络中的内部协变量位移（Internal Covariate Shift），即在网络训练过程中，由于每一层的输入分布不同，导致每一层的参数也不同，从而影响网络的训练效果。规范层通过对输入数据进行归一化处理，可以使得每一层的输入分布更加稳定，从而加速网络的训练过程，提高模型的泛化能力和鲁棒性
```

### 操作

```python
	1.最大最小值归一化（Min-Max Normalization）：将数据的取值范围缩放到[0,1]区间内，公式为x = (x - min)/(max - min)	其中max为数据中的最大值，min为数据中的最小值
    2.归一化
    3.PCA/白化
```

### 规范类型

```python
	1.批量规范化（Batch Normalization）：对每个小批量数据进行归一化处理，使得每个维度的均值为0，方差为1，从而加速训练过程，提高模型的泛化能力
	2.组规范化（Group Normalization）：对每个小组数据进行归一化处理，使得每个小组内的均值为0，方差为1，从而提高模型的鲁棒性，特别是在小批量数据上效果更好
	3.局部响应规范化（Local Response Normalization）：对每个数据点的邻域进行归一化处理，使得每个数据点的响应受到相邻数据点的响应的抑制，从而提高模型的泛化能力和鲁棒性
```

## 全连接层

### 功能

```python
	将前面经过多次卷积、池化、规范化后得到数据，从多维拉成一维（类似把二维数组拉成一维数组），其中每一个元素对应第一层神经元，每一个神经元和后一层所有神经元都相连，后一层的神经元输出结果为与该神经元相连的所有前一层神经元的值乘上他们之间的权重，然后求和后再加上偏置的结果输入到激活函数中得到输出结果
```

### 操作

```python
	使用dropout()函数将一些神经元的结果随机置零，防止出现过拟合现象。如：dropout(network, p=0.5)即每个神经元有50%的几率丢弃输出，相当于Dropout层随机地将一半的神经元的输出设置为0
```

### 示例代码

```python
network = fully_connected(network, 9, activation='softmax')	#第一个参数network为全连接层的输入，第二个参数9为输出层中神经元的数量，第三个参数softmax函数为本全连接层的激活函数
这句代码是搭建的神经网络模型的最后一层，用来输出本次样本的最终结果，输出神经元的数量等于该分类问题的类别数，如：制作猫狗识别模型，最终的输出神经元数量就应该是2，要么是猫要么是狗
```

### 其他

```python
	softmax是一种常用的激活函数，通常用于多分类问题中。softmax函数能够将输出值映射到0到1之间，并使其和为1，因此可以将输出解释为概率分布。就是将输出的结果映射到0-1之间的小数表示
```

### 补充

```python
全连接层的输出一般会让其经过一个ReLU函数后再经过一个Dropout层，用来防止过拟合
```

## 回归层

### 功能

```python
回归层是神经网络的最后一层，用来对模型的输出进行回归预测。判断输出值和实际值之间的差距，差距说明模型做得可以
```

### 优化器

```python
	用来更新模型参数，是得模型输出值逐渐逼近真实值。优化器通过损失函数来调整模型参数，使得模型预测结果更加准确
#常见优化器
    1.随机梯度下降，SGD
    2.自适应据估计，Adam
    3.自适应梯度算法
    4.自适应学习率调整
    5.动量梯度下降，Momentum，如果梯度方向不变，更新就更快，有一定摆脱局部最优解的能力。加速收敛，减少震荡，避免局部最优
```

### 损失函数


```python
	损失函数用来度量预测值的效果
#常见损失函数：
    1.均方误差
    2.平均绝对误差
    3.交叉熵损失函数，适用于分类问题，常用于多分类问题。
```

### 示例代码

```python
network = regression(network, optimizer='momentum', loss='categorical_crossentropy', learning_rate=lr, name='targets')
```

## 其他

### 杂项

```python
1. 卷积层和全连接层都是学习层
2. 除了输入层和输出层，其他的层都叫隐藏层，如卷积层、池化层、全连接层它们都是隐藏层
3. 前向传播计算loss，再反向传播更新参数权重，直到模型收敛
```

### 神经网络每层训练的参数个数计算方法：
```python
   本层的参数个数 = (卷积核的高度 * 卷积核的宽度 * 输入的通道数 + 1) * 卷积核的个数
   #其中：卷积核的高度*宽度*卷积核的个数得出的是卷积核中的参数个数，卷积核个数*1得出的是每个神经元偏置的个数
```
### 卷积核尺寸为什么一般是奇数

```python
	卷积核的大小必须大于1才有提升感受野的作用，所以不能选1；如果卷积核大小为偶数则不能保证输入的特征图和输出的特征图尺寸一致，所以一般选奇数
```

### 每一层卷积需要多少通道（也就是卷积核的数量，因为一个通道就有一个卷积核）、一共需要设置多少层卷积层

```python
	现在没有理论支撑，全是经验之谈，一般选择多少通道既要考虑准确度（卷积核数量越多越准确），又要考虑算力（卷积核越少算得越快）
	需要设置多少层卷积层，目前也是经验之谈，没有理论研究，大家凭感觉搭积木
```

